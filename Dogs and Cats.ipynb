{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#gpu_options = tf.gpu_options(per_process_gpu_memory_fraction=0.3333)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='/home/yifei/Downloads/PetImages'\n",
    "CATEGORIES=['Dog','Cat']\n",
    "IMG_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "x_train=[] #create an empty list for training data\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        my_path=os.path.join(DATA_PATH,category)\n",
    "        classnum=CATEGORIES.index(category) #gives you the index at that category (0 or 1)\n",
    "        \n",
    "        for image in os.listdir(my_path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(my_path,image),cv2.IMREAD_GRAYSCALE) #this will be in grayscale\n",
    "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) #resize every image before passing it in\n",
    "                x_train.append([new_array,classnum])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "\n",
    "create_training_data()\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[152, 154, 154, ..., 245, 245, 246],\n",
      "       [152, 154, 154, ..., 247, 247, 248],\n",
      "       [153, 153, 154, ..., 246, 247, 248],\n",
      "       ...,\n",
      "       [155, 209, 185, ..., 218, 223, 229],\n",
      "       [162, 157, 191, ..., 224, 198, 206],\n",
      "       [187, 156, 151, ..., 210, 222, 220]], dtype=uint8), 1]\n"
     ]
    }
   ],
   "source": [
    "#shuffle data \n",
    "import random\n",
    "random.shuffle(x_train)\n",
    "\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24946, 100, 100, 1)\n",
      "24946\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "x=[]\n",
    "y=[]\n",
    "for features, label in x_train:\n",
    "    x.append(features) \n",
    "    y.append(label) #create lists of features and labels \n",
    "#we cannot pass a list into neural network, we need to use a numpy array\n",
    "x=np.array(x).reshape(-1,IMG_SIZE,IMG_SIZE,1) #1 is for grayscale, -1 represents unknown dimension for np to resolve\n",
    "print(x.shape)\n",
    "\n",
    "#save data\n",
    "pickle_out=open('x.pickle','wb')\n",
    "pickle.dump(x,pickle_out)\n",
    "pickle_out.close();\n",
    "\n",
    "pickle_out=open('y.pickle','wb')\n",
    "pickle.dump(y,pickle_out)\n",
    "pickle_out.close();\n",
    "\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in=open('x.pickle','rb')\n",
    "x=pickle.load(pickle_in)\n",
    "\n",
    "pickle_in=open('y.pickle','rb')\n",
    "y=pickle.load(pickle_in)\n",
    "\n",
    "x=x/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yifei/.conda/envs/ART/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('acc')>0.95):\n",
    "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "        \n",
    "def createModel():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=x.shape[1:]),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(), #flattens the dataset for our input layer\n",
    "        tf.keras.layers.Dense(512,activation=tf.nn.relu), #hidden layer\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid') #sigmoid for final layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "call=myCallback()\n",
    "model=createModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22451 samples, validate on 2495 samples\n",
      "Epoch 1/10\n",
      "22451/22451 [==============================] - 863s 38ms/sample - loss: 0.6153 - acc: 0.6554 - val_loss: 0.5454 - val_acc: 0.7423\n",
      "Epoch 2/10\n",
      "22451/22451 [==============================] - 888s 40ms/sample - loss: 0.4981 - acc: 0.7589 - val_loss: 0.4861 - val_acc: 0.7659\n",
      "Epoch 3/10\n",
      "22451/22451 [==============================] - 730s 33ms/sample - loss: 0.4481 - acc: 0.7883 - val_loss: 0.4968 - val_acc: 0.7647\n",
      "Epoch 4/10\n",
      "22451/22451 [==============================] - 686s 31ms/sample - loss: 0.4067 - acc: 0.8146 - val_loss: 0.4772 - val_acc: 0.7832\n",
      "Epoch 5/10\n",
      "22451/22451 [==============================] - 749s 33ms/sample - loss: 0.3604 - acc: 0.8397 - val_loss: 0.4732 - val_acc: 0.7924\n",
      "Epoch 6/10\n",
      "22451/22451 [==============================] - 531s 24ms/sample - loss: 0.3245 - acc: 0.8588 - val_loss: 0.4862 - val_acc: 0.7812\n",
      "Epoch 7/10\n",
      "22451/22451 [==============================] - 501s 22ms/sample - loss: 0.2645 - acc: 0.8896 - val_loss: 0.5066 - val_acc: 0.7852\n",
      "Epoch 8/10\n",
      "22451/22451 [==============================] - 500s 22ms/sample - loss: 0.2067 - acc: 0.9185 - val_loss: 0.5536 - val_acc: 0.7844\n",
      "Epoch 9/10\n",
      "22451/22451 [==============================] - 551s 25ms/sample - loss: 0.1554 - acc: 0.9424 - val_loss: 0.6029 - val_acc: 0.7816\n",
      "Epoch 10/10\n",
      "22320/22451 [============================>.] - ETA: 3s - loss: 0.1044 - acc: 0.9645\n",
      "Reached 95% accuracy so cancelling training!\n",
      "22451/22451 [==============================] - 649s 29ms/sample - loss: 0.1043 - acc: 0.9646 - val_loss: 0.6933 - val_acc: 0.7972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f16c021b3c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "model.fit(x, y, batch_size=180, validation_split=0.1, epochs=10, callbacks=[call])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats and dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yifei/.conda/envs/ART/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yifei/.conda/envs/ART/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "new_model=tf.keras.models.load_model('cats and dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES=['Dog','Cat']\n",
    "#takes the filepath of an image and resizes it to become an array\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE=100\n",
    "    img_array=cv2.imread(filepath,cv2.IMREAD_GRAYSCALE)\n",
    "    new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) #resize it so it's 100x100\n",
    "    return new_array.reshape(-1,IMG_SIZE,IMG_SIZE,1) #reshape because tf needs this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "dog_array=prepare('dog2.jpeg')\n",
    "prediction=new_model.predict(dog_array)\n",
    "print(CATEGORIES[int(prediction[0][0])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
